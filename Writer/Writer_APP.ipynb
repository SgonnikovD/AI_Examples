{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685180f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastai.text.all import *\n",
    "import gradio as gr\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ec7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import pathlib\n",
    "plt = platform.system()\n",
    "if plt == 'Linux': pathlib.WindowsPath = pathlib.PosixPath\n",
    "if plt == 'Windows': pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "pretrained_weights = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea893d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6787740",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls(file_exts='.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SpacyTokenizerBig():\n",
    "    \"\"\"Измененный класс токенайзера для поддержки словаря большого размера\"\"\"\n",
    "    def __init__(self, lang='en', special_toks=None, buf_sz=5000):\n",
    "        import spacy\n",
    "        from spacy.symbols import ORTH\n",
    "        self.special_toks = ifnone(special_toks, defaults.text_spec_tok)\n",
    "        nlp = spacy.blank(lang)\n",
    "        nlp.max_length = 10**10\n",
    "        for w in self.special_toks: nlp.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.pipe,self.buf_sz = nlp.pipe,buf_sz\n",
    "\n",
    "    def __call__(self, items):\n",
    "        return (L(doc).attrgot('text') for doc in self.pipe(map(str,items), batch_size=self.buf_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23252f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TransformersTokenizer(Transform):\n",
    "    \"\"\"Класс токенайзера для работы с GPT2\"\"\"\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def encodes(self, x): \n",
    "        toks = self.tokenizer.tokenize(x)\n",
    "        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n",
    "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DropOutput(Callback):\n",
    "    \"\"\"Класс поддержки для обучения GPT2\"\"\"\n",
    "    def after_pred(self): self.learn.pred = self.pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "learnHP = load_learner('WriterHP_model_10ep.pkl')\n",
    "learnStK = load_learner('WriterStKng_model_10ep.pkl')\n",
    "\n",
    "learnHP_tr = load_learner('WriterHP_transf_model.pkl')\n",
    "learnStK_tr = load_learner('WriterStKng_transf_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def write_text(text, neuronet, author, words_cnt):\n",
    "    \"\"\"Генерирует текст в соответствии с заданными параметрами.\n",
    "    Возвращает сгенерированный текст\"\"\"\n",
    "    words_cnt=int(words_cnt)\n",
    "    if neuronet == 'GPT2':\n",
    "        prompt_ids = tokenizer.encode(text)\n",
    "        inp = tensor(prompt_ids)[None]\n",
    "        if author == 'Harry Potter Style':\n",
    "            preds = learnHP_tr.model.generate(inp, max_length=words_cnt, repetition_penalty=6.0,\n",
    "                                              temperature=1.5, no_repeat_ngram_size=2,\n",
    "                                              do_sample=True, top_k=5, top_p=0.95)\n",
    "        elif author == 'Stephen King Style':\n",
    "            preds = learnStK_tr.model.generate(inp, max_length=words_cnt, repetition_penalty=6.0,\n",
    "                                              temperature=1.5, no_repeat_ngram_size=2,\n",
    "                                              do_sample=True, top_k=5, top_p=0.95)        \n",
    "        new_text = tokenizer.decode(preds[0].numpy())\n",
    "    elif neuronet == 'AWD_LSTM':\n",
    "        if author == 'Harry Potter Style':\n",
    "            preds = [learnHP.predict(text, words_cnt, temperature=1)]\n",
    "        elif author == 'Stephen King Style':\n",
    "            preds = [learnStK.predict(text, words_cnt, temperature=1)]\n",
    "        new_text = \"\\n\".join(preds)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de748f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "input_text = gr.components.Textbox(value='It was a bright day.', label='Input text')\n",
    "label = gr.components.Textbox()\n",
    "wordcount_num = gr.components.Number(value=40, label='New tokens count')\n",
    "neuro_radio = gr.components.Radio(choices=[\"AWD_LSTM\", \"GPT2\"], value=\"GPT2\", label='NeuroNet Type')\n",
    "\n",
    "author_radio = gr.components.Radio(choices=[\"Harry Potter Style\", \"Stephen King Style\"],\n",
    "                                   value=\"Harry Potter Style\", label='Author')\n",
    "\n",
    "intface = gr.Interface(fn=write_text, inputs=[input_text, neuro_radio, author_radio, wordcount_num], outputs=label)\n",
    "intface.launch(inline=False, server_name='0.0.0.0', server_port=12250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b58b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10234b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Экспорт файла _APP.ipynb для загрузки на Hugging Face\n",
    "from nbdev.export import nb_export\n",
    "nb_export('Writer_APP.ipynb','./')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
